{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de9ce678-352e-40dd-abaa-97c703f58425",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad85f8fc-7dde-45f3-8c70-0f313a27e14a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------------+\n| company|sales|perunitprice|\n+--------+-----+------------+\n|  amazon| 1000|         300|\n|flipkart|  400|         249|\n|    ajio|  490|         400|\n|  myntra| 2400|        2300|\n+--------+-----+------------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [('amazon','1000' , '300'),\n",
    "            ('flipkart','400' , '249'),\n",
    "       ('ajio','490' , '400'),\n",
    "       ('myntra','2400' , '2300'),\n",
    "       \n",
    "       ]\n",
    "schema = ['company' , 'sales','perunitprice']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0101fc52-e801-47ce-9736-2cc0602e2bb3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------------+---------+\n| company|sales|perunitprice|  Revenue|\n+--------+-----+------------+---------+\n|  amazon| 1000|         300| 300000.0|\n|flipkart|  400|         249|  99600.0|\n|    ajio|  490|         400| 196000.0|\n|  myntra| 2400|        2300|5520000.0|\n+--------+-----+------------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "df_revenue = df.select(df.company, df.sales,df.perunitprice, (df['sales']*df['perunitprice']).alias(\"Revenue\"))\n",
    "df_revenue.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d782c183-1f4b-43bd-a462-31c12f299ce3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+\n| company|count(revenue)|\n+--------+--------------+\n|  amazon|             1|\n|flipkart|             1|\n|    ajio|             1|\n|  myntra|             1|\n+--------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, max, min \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aabeab01-7af8-4301-bcf4-3c344c8a0533",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n| company|revenue|\n+--------+-------+\n|  amazon|   1000|\n|flipkart|    400|\n|    ajio|    490|\n|  myntra|   2400|\n|  amazon|  31000|\n|flipkart|   4200|\n|    ajio|   1490|\n|  myntra|  23400|\n+--------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "data2 =  [('amazon','1000' ),\n",
    "            ('flipkart','400'),\n",
    "       ('ajio','490' ),\n",
    "       ('myntra','2400'),\n",
    "        ('amazon','31000' ),\n",
    "            ('flipkart','4200'),\n",
    "       ('ajio','1490' ),\n",
    "       ('myntra','23400'),\n",
    "          \n",
    "       \n",
    "       ]\n",
    "schema = ['company' , 'revenue' ]\n",
    "\n",
    "df = spark.createDataFrame(data2, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b89bdd96-3a28-4b94-8eb2-6e31faefde35",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n| company|revenue|\n+--------+-------+\n|  amazon|   1000|\n|flipkart|    400|\n|    ajio|    490|\n|  myntra|   2400|\n|  amazon|  31000|\n|flipkart|   4200|\n|    ajio|   1490|\n|  myntra|  23400|\n+--------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df_revenue = df.select(df.company,df.revenue).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "229cd25f-1bf4-4966-98d6-2b8a721c84a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n| company|revenue|\n+--------+-------+\n|  amazon|   1000|\n|flipkart|    400|\n|    ajio|    490|\n|  myntra|   2400|\n|  amazon|  31000|\n|flipkart|   4200|\n|    ajio|   1490|\n|  myntra|  23400|\n+--------+-------+\n\nroot\n |-- company: string (nullable = true)\n |-- revenue: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_revenue = df.withColumn('revenue' , df.revenue.cast(\"Integer\")).show()\n",
    "df = df.withColumn('revenue',df.revenue.cast('Integer'))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd63ab17-a459-4a95-9e9f-02e26a73135e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+\n| company|total_revenue|\n+--------+-------------+\n|  amazon|        32000|\n|flipkart|         4600|\n|    ajio|         1980|\n|  myntra|        25800|\n+--------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.functions import col \n",
    "\n",
    "df = df.withColumn(\"revenue\", col(\"revenue\").cast(\"int\"))\n",
    "group_by = df.groupBy('company').agg(sum(df['revenue'].cast('int')).alias('total_revenue')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b03a3ce8-06ba-4036-8350-729101d5178f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n| company|revenue|\n+--------+-------+\n|  amazon|  14224|\n|flipkart|  11224|\n|  myntra|  24224|\n|    ajio|  34224|\n|  amazon|  15224|\n|flipkart|  11224|\n|    ajio|  19224|\n|  myntra|  23224|\n|  amazon|  10224|\n|  myntra|  94224|\n|flipkart|  32224|\n|    ajio|  39224|\n|  myntra|  12324|\n+--------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [('amazon',14224),\n",
    "       ('flipkart',11224),\n",
    "       ('myntra',24224),\n",
    "       ('ajio',34224),\n",
    "       ('amazon',15224),\n",
    "       ('flipkart',11224),('ajio',19224),\n",
    "       ('myntra',23224),\n",
    "       ('amazon',10224),\n",
    "       ('myntra',94224),\n",
    "       ('flipkart',32224),('ajio',39224),('myntra',12324),]\n",
    "schema = ['company','revenue']\n",
    "df = spark.createDataFrame(data,schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcbae74f-28d7-44e7-b32e-fdff183e96bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- company: string (nullable = true)\n |-- revenue: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "#checking the datatype\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2133d311-9993-4020-9172-dd388a1766af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+\n| company|total_revenue|\n+--------+-------------+\n|  amazon|        39672|\n|flipkart|        54672|\n|  myntra|       153996|\n|    ajio|        92672|\n+--------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "#converting the datatype of revenue to long int\n",
    "from pyspark.sql.functions import col\n",
    "company_revenue = df.groupBy('company').agg(sum('revenue').alias('total_revenue')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be907b06-9a01-47eb-9c9c-29a33b58de15",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data =  [  (1, 'amazon'),\n",
    "           (2, 'flipkart'),\n",
    "           (3, 'myntra'),\n",
    "         (4,'google'),\n",
    "         (5,'microsoft')\n",
    "        ]\n",
    "schema = ['id','company']\n",
    "df1 = spark.createDataFrame(data,schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a0ef885-ad98-4783-a49d-05d39c879409",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [('bold',12323),\n",
    "       ('stylic',32332),\n",
    "       ('italic',42442),\n",
    "       ('normal',23221),\n",
    "       ('normal',123231)\n",
    "       ]\n",
    "schema = ['body_style','price']\n",
    "df2 = spark.createDataFrame(data,schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bcd70e7-4c46-44f0-b812-85dc26e4a486",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n| id|  company|\n+---+---------+\n|  1|   amazon|\n|  2| flipkart|\n|  3|   myntra|\n|  4|   google|\n|  5|microsoft|\n+---+---------+\n\n+----------+------+\n|body_style| price|\n+----------+------+\n|      bold| 12323|\n|    stylic| 32332|\n|    italic| 42442|\n|    normal| 23221|\n|    normal|123231|\n+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df1.show()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14d98e07-6a91-4f0a-9f65-9af6af934e1f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+----------+------+\n|  id|  company|body_style| price|\n+----+---------+----------+------+\n|   1|   amazon|      null|  null|\n|   2| flipkart|      null|  null|\n|   3|   myntra|      null|  null|\n|   4|   google|      null|  null|\n|   5|microsoft|      null|  null|\n|null|     null|      bold| 12323|\n|null|     null|    stylic| 32332|\n|null|     null|    italic| 42442|\n|null|     null|    normal| 23221|\n|null|     null|    normal|123231|\n+----+---------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "unionByName = df1.unionByName(df2,allowMissingColumns= True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d9898ae-c7ab-4418-ac52-7f87489b13a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-----------+---------------+-----+\n|company|engine type|horse power|average mileage|price|\n+-------+-----------+-----------+---------------+-----+\n|    BMW|   Gasoline|        332|            212|60000|\n|  Tesla|   Electric|        400|            250|80000|\n| Toyota|     Hybrid|        200|            400|35000|\n|   Ford|   Gasoline|        250|            300|45000|\n|  Honda|   Gasoline|        180|            350|30000|\n+-------+-----------+-----------+---------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = [ ('BMW', 'Gasoline', 332, 212, 60000),\n",
    "    \n",
    "          ('Tesla', 'Electric', 400, 250, 80000),\n",
    "       \n",
    "        ('Toyota', 'Hybrid', 200, 400, 35000),\n",
    "        ('Ford', 'Gasoline', 250, 300, 45000),\n",
    "        ('Honda', 'Gasoline', 180, 350, 30000)]\n",
    "\n",
    "schema = ['company', 'engine type', 'horse power', 'average mileage', 'price']\n",
    "\n",
    "# Create the DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f2dfd5f-c1f4-4908-bf21-5c456228e302",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n|company|\n+-------+\n|    BMW|\n|  Tesla|\n| Toyota|\n|   Ford|\n|  Honda|\n+-------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "company = df.select(col('company')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b277abb-8847-46f5-9ab2-920985225096",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n|engine type|\n+-----------+\n|   Gasoline|\n|   Electric|\n|     Hybrid|\n|   Gasoline|\n|   Gasoline|\n+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "engine_type = df.select(col('engine type')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "232c7b68-c753-4984-b853-981272a51ab2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n|horse power|\n+-----------+\n|        332|\n|        400|\n|        200|\n|        250|\n|        180|\n+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "horse_power = df.select(col('horse power')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8369df8d-d455-4070-bfe9-75cdf160cd89",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n|average mileage|\n+---------------+\n|            212|\n|            250|\n|            400|\n|            300|\n|            350|\n+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "average_mileage =df.select(col('average mileage')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f7e62fb-47d1-4f4d-b135-815f5b440cc2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n|price|\n+-----+\n|60000|\n|80000|\n|35000|\n|45000|\n|30000|\n+-----+\n\n"
     ]
    }
   ],
   "source": [
    "price = df.select(col('price')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "508a8d83-f25c-4e42-afbb-bcf94be51b9b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+------+---+\n| id|     name|salary|dep|\n+---+---------+------+---+\n|  1|bhupendra| 20001|  2|\n|  2|    rahul| 23124|  1|\n|  3|    rohan| 32332|  4|\n+---+---------+------+---+\n\n+---+-------+\n| id|depName|\n+---+-------+\n|  1|     IT|\n|  2|     HR|\n|  3|Payroll|\n+---+-------+\n\n"
     ]
    }
   ],
   "source": [
    "data1 = [(1,'bhupendra',20001,2), (2,'rahul',23124,1),(3,'rohan',32332,4)]\n",
    "schema1 = [ 'id','name','salary','dep']\n",
    "data2 = [(1 , 'IT'), (2,'HR'),(3,'Payroll')]\n",
    "schema2= ['id','depName']\n",
    "\n",
    "emp = spark.createDataFrame(data1,schema1)\n",
    "dep  =spark.createDataFrame(data2,schema2)\n",
    "emp.show()\n",
    "dep.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e80db94d-91f4-429e-9536-328972334a84",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+------+---+---+-------+\n| id|     name|salary|dep| id|depName|\n+---+---------+------+---+---+-------+\n|  2|    rahul| 23124|  1|  1|     IT|\n|  1|bhupendra| 20001|  2|  2|     HR|\n+---+---------+------+---+---+-------+\n\n"
     ]
    }
   ],
   "source": [
    "innerjoined = emp.join(dep,emp.dep== dep.id,'inner')\n",
    "innerjoined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b8cd6ed-7fa0-4cd8-9bc1-c2137449f047",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+------+---+----+-------+\n| id|     name|salary|dep|  id|depName|\n+---+---------+------+---+----+-------+\n|  1|bhupendra| 20001|  2|   2|     HR|\n|  2|    rahul| 23124|  1|   1|     IT|\n|  3|    rohan| 32332|  4|null|   null|\n+---+---------+------+---+----+-------+\n\n"
     ]
    }
   ],
   "source": [
    "emp.join(dep,emp.dep== dep.id,'left').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "818f4303-271a-47b2-ae0a-3bcbe4dd72a9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+---+\n| id| name|salary|dep|\n+---+-----+------+---+\n|  3|rohan| 32332|  4|\n+---+-----+------+---+\n\n"
     ]
    }
   ],
   "source": [
    "emp.join(dep,emp.dep == dep.id , 'leftanti').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "675ecfe1-438d-470c-866a-e23eac18b171",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+------+---+---+-------+\n| id|     name|salary|dep| id|depName|\n+---+---------+------+---+---+-------+\n|  2|    rahul| 23124|  1|  1|     IT|\n|  1|bhupendra| 20001|  2|  2|     HR|\n+---+---------+------+---+---+-------+\n\n"
     ]
    }
   ],
   "source": [
    "innerjoined.createOrReplaceTempView('Employees')\n",
    "df1 = spark.sql(\" select * from Employees \")\n",
    "df1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b74f431f-6c61-499a-ba44-4bb4e2f6b071",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>salary</th><th>dep</th><th>id</th><th>depName</th></tr></thead><tbody><tr><td>2</td><td>rahul</td><td>23124</td><td>1</td><td>1</td><td>IT</td></tr><tr><td>1</td><td>bhupendra</td><td>20001</td><td>2</td><td>2</td><td>HR</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         2,
         "rahul",
         23124,
         1,
         1,
         "IT"
        ],
        [
         1,
         "bhupendra",
         20001,
         2,
         2,
         "HR"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "dep",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "depName",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from Employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b14b78b-b956-48ed-adc7-c964b9f56fc9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-----------+---------------+-----+\n|company|engine type|horse power|average mileage|price|\n+-------+-----------+-----------+---------------+-----+\n|    BMW|   Gasoline|       null|            212|60000|\n|  Tesla|       null|        400|            250|80000|\n| Toyota|     Hybrid|        200|            400|35000|\n|   null|   Gasoline|        250|            300|45000|\n|  Honda|   Gasoline|        180|           null|30000|\n+-------+-----------+-----------+---------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = [ ('BMW', 'Gasoline', None , 212, 60000),\n",
    "    \n",
    "          ('Tesla', None, 400, 250, 80000),\n",
    "       \n",
    "        ('Toyota', 'Hybrid', 200, 400, 35000),\n",
    "        (None, 'Gasoline', 250, 300, 45000),\n",
    "        ('Honda', 'Gasoline', 180,None, 30000)]\n",
    "\n",
    "schema = ['company', 'engine type', 'horse power', 'average mileage', 'price']\n",
    "\n",
    "# Create the DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "432f9d7d-05f0-4377-97a7-3a4cc259eb2d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-----------+---------------+-----+\n|company|engine type|horse power|average mileage|price|\n+-------+-----------+-----------+---------------+-----+\n|    BMW|   Gasoline|       null|            212|60000|\n|  Tesla|    unknown|        400|            250|80000|\n| Toyota|     Hybrid|        200|            400|35000|\n|unknown|   Gasoline|        250|            300|45000|\n|  Honda|   Gasoline|        180|           null|30000|\n+-------+-----------+-----------+---------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df2 = df.fillna('unknown')\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03b2d83a-28ec-4c43-a981-7ed0640195a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- company: string (nullable = false)\n |-- engine type: string (nullable = false)\n |-- horse power: long (nullable = true)\n |-- average mileage: long (nullable = true)\n |-- price: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64d2cd9d-6a4c-4d5b-bcb5-a5745666aae8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-----+\n|horse power|average mileage|price|\n+-----------+---------------+-----+\n|       null|            212|60000|\n|        400|            250|80000|\n|        200|            400|35000|\n|        250|            300|45000|\n|        180|           null|30000|\n+-----------+---------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df2 = df.select(df['horse power'],df['average mileage'], df['price'] ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a09af29b-99fb-4c5c-a06a-cdb091512f8c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-----------+---------------+-----+\n|company|engine type|horse power|average mileage|price|\n+-------+-----------+-----------+---------------+-----+\n|    BMW|   Gasoline|          0|            212|60000|\n|  Tesla|    unknown|        400|            250|80000|\n| Toyota|     Hybrid|        200|            400|35000|\n|unknown|   Gasoline|        250|            300|45000|\n|  Honda|   Gasoline|        180|              0|30000|\n+-------+-----------+-----------+---------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df_filled = df.fillna(0).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66769cc8-02d2-4e7a-a1e3-8686f2b35c15",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df =df.withColumn('horse power', df['horse power'].cast('string'))\n",
    "df =df.withColumn('average mileage', df['average mileage'].cast('string'))\n",
    "df =df.withColumn('price', df['price'].cast('string'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff207e6c-8e45-4a51-9a1e-2b921bb85008",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- company: string (nullable = true)\n |-- engine type: string (nullable = true)\n |-- horse power: string (nullable = true)\n |-- average mileage: string (nullable = true)\n |-- price: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d472e54-d2d9-47e7-9eb5-56386ecd23bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-----------+---------------+-----+\n|company|engine type|horse power|average mileage|price|\n+-------+-----------+-----------+---------------+-----+\n|    BMW|   Gasoline|    unknown|            212|60000|\n|  Tesla|    unknown|        400|            250|80000|\n| Toyota|     Hybrid|        200|            400|35000|\n|unknown|   Gasoline|        250|            300|45000|\n|  Honda|   Gasoline|        180|        unknown|30000|\n+-------+-----------+-----------+---------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df_fill = df.fillna('unknown').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84824d96-d6e3-47b7-83d9-519f2f16c761",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.select(*(col(c).cast('int') for c in df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcea2714-fd4e-4072-a111-124826c5005f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- company: integer (nullable = true)\n |-- engine type: integer (nullable = true)\n |-- horse power: integer (nullable = true)\n |-- average mileage: integer (nullable = true)\n |-- price: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3a271c6-42f1-4031-9ede-30e413e96d0a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceGlobalTempView('employeeglobal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3e6d69a-ce04-4482-9518-d1e410d68a83",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>company</th><th>engine type</th><th>horse power</th><th>average mileage</th><th>price</th></tr></thead><tbody><tr><td>null</td><td>null</td><td>null</td><td>212</td><td>60000</td></tr><tr><td>null</td><td>null</td><td>400</td><td>250</td><td>80000</td></tr><tr><td>null</td><td>null</td><td>200</td><td>400</td><td>35000</td></tr><tr><td>null</td><td>null</td><td>250</td><td>300</td><td>45000</td></tr><tr><td>null</td><td>null</td><td>180</td><td>null</td><td>30000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         null,
         null,
         null,
         212,
         60000
        ],
        [
         null,
         null,
         400,
         250,
         80000
        ],
        [
         null,
         null,
         200,
         400,
         35000
        ],
        [
         null,
         null,
         250,
         300,
         45000
        ],
        [
         null,
         null,
         180,
         null,
         30000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "company",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "engine type",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "horse power",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "average mileage",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "price",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from global_temp.employeeglobal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3dcf624b-a15a-4bad-a663-19338e6a1929",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "030ed38b-dc5d-4e32-a508-22fefe9b6de0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_practice  = spark.read.csv('dbfs:/FileStore/Customer_Pratice.csv')\n",
    "order_practice = spark.read.csv('dbfs:/FileStore/Orders_Pratice.csv')\n",
    "order_details_practice = spark.read.csv('dbfs:/FileStore/Order_Details_Pratice.csv')\n",
    "product_practice = spark.read.csv('dbfs:/FileStore/Products_Pratice.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff8f80b7-a360-4c0e-a1bd-25756f9f2eec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------+--------------------+----------------+\n|       _c0|      _c1|     _c2|                 _c3|             _c4|\n+----------+---------+--------+--------------------+----------------+\n|CustomerID|FirstName|LastName|        EmailAddress|     PhoneNumber|\n|     cust1|     John|   Smith|john.smith@exampl...|    555-123-4567|\n|     cust2|     Jane|     Doe|jane.doe@example.com|    555-987-6543|\n|     cust3|  Michael| Johnson|michael.johnson@e...|    555-555-1212|\n|     cust4|    Sarah|     Lee|sarah.lee@example...|    555-555-1212|\n|     cust5|    David|     Kim|david.kim@example...|    908-434-3434|\n|     cust6|    wonda|    long|wonda.long@exampl...|    322-545-1112|\n|     cust7|    shuri|     wun|shuri.wun@example...|354-566-456-2345|\n|     cust8|      tom| holland|tom.holland@examp...|    123-456-7890|\n+----------+---------+--------+--------------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "customer_practice.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd41555f-6f30-46d6-b70c-d7d8db4a8a63",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3125408404689992>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m result_df \u001B[38;5;241m=\u001B[39m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\"\"\u001B[39;49m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;124;43m    SELECT c._c1 AS FirstName, c._c2 AS LastName, SUM(p.Price * od._c2) AS TotalRevenue\u001B[39;49m\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;124;43m    FROM customer c\u001B[39;49m\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;124;43m    JOIN `order` o ON c._c0 = o.CustomerID\u001B[39;49m\n",
       "\u001B[1;32m      5\u001B[0m \u001B[38;5;124;43m    JOIN order_details od ON o._c0 = od.OrderID\u001B[39;49m\n",
       "\u001B[1;32m      6\u001B[0m \u001B[38;5;124;43m    JOIN product p ON od._c1 = p._c0\u001B[39;49m\n",
       "\u001B[1;32m      7\u001B[0m \u001B[38;5;124;43m    GROUP BY c._c0, c._c1, c._c2\u001B[39;49m\n",
       "\u001B[1;32m      8\u001B[0m \u001B[38;5;124;43m\"\"\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1387\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1385\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m   1386\u001B[0m     litArgs \u001B[38;5;241m=\u001B[39m {k: _to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m {})\u001B[38;5;241m.\u001B[39mitems()}\n",
       "\u001B[0;32m-> 1387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n",
       "\u001B[1;32m   1388\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m   1389\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `o`.`CustomerID` cannot be resolved. Did you mean one of the following? [`o`.`_c0`, `o`.`_c1`, `o`.`_c2`, `o`.`_c3`, `c`.`_c0`].; line 4 pos 30;\n",
       "'Aggregate ['c._c0, 'c._c1, 'c._c2], ['c._c1 AS FirstName#754, 'c._c2 AS LastName#755, 'SUM(('p.Price * 'od._c2)) AS TotalRevenue#756]\n",
       "+- 'Join Inner, ('od._c1 = 'p._c0)\n",
       "   :- 'Join Inner, ('o._c0 = 'od.OrderID)\n",
       "   :  :- 'Join Inner, (_c0#93 = 'o.CustomerID)\n",
       "   :  :  :- SubqueryAlias c\n",
       "   :  :  :  +- SubqueryAlias customer\n",
       "   :  :  :     +- View (`customer`, [_c0#93,_c1#94,_c2#95,_c3#96,_c4#97])\n",
       "   :  :  :        +- Relation [_c0#93,_c1#94,_c2#95,_c3#96,_c4#97] csv\n",
       "   :  :  +- SubqueryAlias o\n",
       "   :  :     +- SubqueryAlias order\n",
       "   :  :        +- View (`order`, [_c0#120,_c1#121,_c2#122,_c3#123])\n",
       "   :  :           +- Relation [_c0#120,_c1#121,_c2#122,_c3#123] csv\n",
       "   :  +- SubqueryAlias od\n",
       "   :     +- SubqueryAlias order_details\n",
       "   :        +- View (`order_details`, [_c0#145,_c1#146,_c2#147])\n",
       "   :           +- Relation [_c0#145,_c1#146,_c2#147] csv\n",
       "   +- SubqueryAlias p\n",
       "      +- SubqueryAlias product\n",
       "         +- View (`product`, [_c0#168,_c1#169,_c2#170,_c3#171])\n",
       "            +- Relation [_c0#168,_c1#169,_c2#170,_c3#171] csv\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-3125408404689992>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m result_df \u001B[38;5;241m=\u001B[39m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\"\"\u001B[39;49m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;124;43m    SELECT c._c1 AS FirstName, c._c2 AS LastName, SUM(p.Price * od._c2) AS TotalRevenue\u001B[39;49m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;124;43m    FROM customer c\u001B[39;49m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;124;43m    JOIN `order` o ON c._c0 = o.CustomerID\u001B[39;49m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;124;43m    JOIN order_details od ON o._c0 = od.OrderID\u001B[39;49m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;124;43m    JOIN product p ON od._c1 = p._c0\u001B[39;49m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;124;43m    GROUP BY c._c0, c._c1, c._c2\u001B[39;49m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;124;43m\"\"\"\u001B[39;49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1387\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n\u001B[1;32m   1385\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1386\u001B[0m     litArgs \u001B[38;5;241m=\u001B[39m {k: _to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m {})\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[0;32m-> 1387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m   1388\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   1389\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `o`.`CustomerID` cannot be resolved. Did you mean one of the following? [`o`.`_c0`, `o`.`_c1`, `o`.`_c2`, `o`.`_c3`, `c`.`_c0`].; line 4 pos 30;\n'Aggregate ['c._c0, 'c._c1, 'c._c2], ['c._c1 AS FirstName#754, 'c._c2 AS LastName#755, 'SUM(('p.Price * 'od._c2)) AS TotalRevenue#756]\n+- 'Join Inner, ('od._c1 = 'p._c0)\n   :- 'Join Inner, ('o._c0 = 'od.OrderID)\n   :  :- 'Join Inner, (_c0#93 = 'o.CustomerID)\n   :  :  :- SubqueryAlias c\n   :  :  :  +- SubqueryAlias customer\n   :  :  :     +- View (`customer`, [_c0#93,_c1#94,_c2#95,_c3#96,_c4#97])\n   :  :  :        +- Relation [_c0#93,_c1#94,_c2#95,_c3#96,_c4#97] csv\n   :  :  +- SubqueryAlias o\n   :  :     +- SubqueryAlias order\n   :  :        +- View (`order`, [_c0#120,_c1#121,_c2#122,_c3#123])\n   :  :           +- Relation [_c0#120,_c1#121,_c2#122,_c3#123] csv\n   :  +- SubqueryAlias od\n   :     +- SubqueryAlias order_details\n   :        +- View (`order_details`, [_c0#145,_c1#146,_c2#147])\n   :           +- Relation [_c0#145,_c1#146,_c2#147] csv\n   +- SubqueryAlias p\n      +- SubqueryAlias product\n         +- View (`product`, [_c0#168,_c1#169,_c2#170,_c3#171])\n            +- Relation [_c0#168,_c1#169,_c2#170,_c3#171] csv\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `o`.`CustomerID` cannot be resolved. Did you mean one of the following? [`o`.`_c0`, `o`.`_c1`, `o`.`_c2`, `o`.`_c3`, `c`.`_c0`].; line 4 pos 30;\n'Aggregate ['c._c0, 'c._c1, 'c._c2], ['c._c1 AS FirstName#754, 'c._c2 AS LastName#755, 'SUM(('p.Price * 'od._c2)) AS TotalRevenue#756]\n+- 'Join Inner, ('od._c1 = 'p._c0)\n   :- 'Join Inner, ('o._c0 = 'od.OrderID)\n   :  :- 'Join Inner, (_c0#93 = 'o.CustomerID)\n   :  :  :- SubqueryAlias c\n   :  :  :  +- SubqueryAlias customer\n   :  :  :     +- View (`customer`, [_c0#93,_c1#94,_c2#95,_c3#96,_c4#97])\n   :  :  :        +- Relation [_c0#93,_c1#94,_c2#95,_c3#96,_c4#97] csv\n   :  :  +- SubqueryAlias o\n   :  :     +- SubqueryAlias order\n   :  :        +- View (`order`, [_c0#120,_c1#121,_c2#122,_c3#123])\n   :  :           +- Relation [_c0#120,_c1#121,_c2#122,_c3#123] csv\n   :  +- SubqueryAlias od\n   :     +- SubqueryAlias order_details\n   :        +- View (`order_details`, [_c0#145,_c1#146,_c2#147])\n   :           +- Relation [_c0#145,_c1#146,_c2#147] csv\n   +- SubqueryAlias p\n      +- SubqueryAlias product\n         +- View (`product`, [_c0#168,_c1#169,_c2#170,_c3#171])\n            +- Relation [_c0#168,_c1#169,_c2#170,_c3#171] csv\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "result_df = spark.sql(\"\"\"\n",
    "    SELECT c._c1 AS FirstName, c._c2 AS LastName, SUM(p.Price * od._c2) AS TotalRevenue\n",
    "    FROM customer c\n",
    "    JOIN `order` o ON c._c0 = o.CustomerID\n",
    "    JOIN order_details od ON o._c0 = od.OrderID\n",
    "    JOIN product p ON od._c1 = p._c0\n",
    "    GROUP BY c._c0, c._c1, c._c2\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61b9851d-833c-4d5c-9b41-38eed9835a71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+----------+\n|    _c0|       _c1|       _c2|       _c3|\n+-------+----------+----------+----------+\n|OrderID|CustomerID|LocationID| OrderDate|\n|    Or1|     cust2|        L1|2022-02-22|\n|    Or2|     cust4|        L2|2022-02-23|\n|    Or3|     cust3|        L3|2022-02-24|\n|    Or4|     cust1|        L3|2022-02-25|\n|    Or5|     cust4|        L4|2022-02-26|\n|    Or6|     cust2|        L5|2022-02-24|\n|    Or7|     cust5|        L6|2022-02-22|\n+-------+----------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "order_practice.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd93c871-8e27-40de-b38e-e162f5b372d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------+\n|    _c0|      _c1|     _c2|\n+-------+---------+--------+\n|OrderID|ProductID|Quantity|\n|    Or1|       P1|      10|\n|    Or3|       P3|       2|\n|    Or2|      P10|       5|\n|    Or7|       P1|       4|\n|    Or5|       P3|       5|\n|    Or2|       P5|       8|\n|    Or6|       P9|       5|\n+-------+---------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "order_details_practice.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95ff5848-f167-44b0-82a6-90fc6d5b27f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+----------+-----+\n|      _c0|        _c1|       _c2|  _c3|\n+---------+-----------+----------+-----+\n|ProductID|ProductName|  Category|Price|\n|       P1|     Apples|    Fruits|  199|\n|       P2|    Bananas|    Fruits|   99|\n|       P3|    Oranges|    Fruits|   29|\n|       P4|   Tomatoes|Vegetables|   25|\n|       P5|  Cucumbers|Vegetables|   99|\n|       P6|    Carrots|Vegetables|   75|\n|       P7|      Bread|    Bakery|   32|\n|       P8|       Milk|     Dairy|   25|\n|       P9|     Cheese|     Dairy|   40|\n|      P10|       Eggs|     Dairy|    8|\n+---------+-----------+----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "product_practice.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "579ecac5-d9bc-4fb4-ad37-1b6dc12c6404",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3125408404689996>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m customer_table \u001B[38;5;241m=\u001B[39m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mselect * from customer_practice\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1387\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1385\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m   1386\u001B[0m     litArgs \u001B[38;5;241m=\u001B[39m {k: _to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m {})\u001B[38;5;241m.\u001B[39mitems()}\n",
       "\u001B[0;32m-> 1387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n",
       "\u001B[1;32m   1388\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m   1389\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `customer_practice` cannot be found. Verify the spelling and correctness of the schema and catalog.\n",
       "If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\n",
       "To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\n",
       "'Project [*]\n",
       "+- 'UnresolvedRelation [customer_practice], [], false\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-3125408404689996>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m customer_table \u001B[38;5;241m=\u001B[39m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mselect * from customer_practice\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1387\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n\u001B[1;32m   1385\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1386\u001B[0m     litArgs \u001B[38;5;241m=\u001B[39m {k: _to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m {})\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[0;32m-> 1387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m   1388\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   1389\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `customer_practice` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [customer_practice], [], false\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `customer_practice` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [customer_practice], [], false\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "customer_table = spark.sql(\"select * from customer_practice\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b546fd5a-6b57-4d18-9c0a-7714169b7606",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>_c0</th><th>_c1</th><th>_c2</th><th>_c3</th></tr></thead><tbody><tr><td>OrderID</td><td>CustomerID</td><td>LocationID</td><td>OrderDate</td></tr><tr><td>Or1</td><td>cust2</td><td>L1</td><td>2022-02-22</td></tr><tr><td>Or2</td><td>cust4</td><td>L2</td><td>2022-02-23</td></tr><tr><td>Or3</td><td>cust3</td><td>L3</td><td>2022-02-24</td></tr><tr><td>Or4</td><td>cust1</td><td>L3</td><td>2022-02-25</td></tr><tr><td>Or5</td><td>cust4</td><td>L4</td><td>2022-02-26</td></tr><tr><td>Or6</td><td>cust2</td><td>L5</td><td>2022-02-24</td></tr><tr><td>Or7</td><td>cust5</td><td>L6</td><td>2022-02-22</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "OrderID",
         "CustomerID",
         "LocationID",
         "OrderDate"
        ],
        [
         "Or1",
         "cust2",
         "L1",
         "2022-02-22"
        ],
        [
         "Or2",
         "cust4",
         "L2",
         "2022-02-23"
        ],
        [
         "Or3",
         "cust3",
         "L3",
         "2022-02-24"
        ],
        [
         "Or4",
         "cust1",
         "L3",
         "2022-02-25"
        ],
        [
         "Or5",
         "cust4",
         "L4",
         "2022-02-26"
        ],
        [
         "Or6",
         "cust2",
         "L5",
         "2022-02-24"
        ],
        [
         "Or7",
         "cust5",
         "L6",
         "2022-02-22"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "_c0",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_c1",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_c2",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_c3",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a087c93a-fbe8-4a18-bd0c-8b89fd50b531",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "order_details = order_details.createOrReplaceTempView(\"order_details\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3db17ada-780e-40b1-b6b4-1f4e022bd2a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>_c0</th><th>_c1</th><th>_c2</th></tr></thead><tbody><tr><td>OrderID</td><td>ProductID</td><td>Quantity</td></tr><tr><td>Or1</td><td>P1</td><td>10</td></tr><tr><td>Or3</td><td>P3</td><td>2</td></tr><tr><td>Or2</td><td>P10</td><td>5</td></tr><tr><td>Or7</td><td>P1</td><td>4</td></tr><tr><td>Or5</td><td>P3</td><td>5</td></tr><tr><td>Or2</td><td>P5</td><td>8</td></tr><tr><td>Or6</td><td>P9</td><td>5</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "OrderID",
         "ProductID",
         "Quantity"
        ],
        [
         "Or1",
         "P1",
         "10"
        ],
        [
         "Or3",
         "P3",
         "2"
        ],
        [
         "Or2",
         "P10",
         "5"
        ],
        [
         "Or7",
         "P1",
         "4"
        ],
        [
         "Or5",
         "P3",
         "5"
        ],
        [
         "Or2",
         "P5",
         "8"
        ],
        [
         "Or6",
         "P9",
         "5"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "_c0",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_c1",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_c2",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from order_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5d52ca4-c32b-4fbc-8aab-579cb34c5d19",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "product = product.createOrReplaceTempView(\"product\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59807e6b-0f06-4be7-a592-5aec692d1a10",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>_c0</th><th>_c1</th><th>_c2</th><th>_c3</th></tr></thead><tbody><tr><td>ProductID</td><td>ProductName</td><td>Category</td><td>Price</td></tr><tr><td>P1</td><td>Apples</td><td>Fruits</td><td>199</td></tr><tr><td>P2</td><td>Bananas</td><td>Fruits</td><td>99</td></tr><tr><td>P3</td><td>Oranges</td><td>Fruits</td><td>29</td></tr><tr><td>P4</td><td>Tomatoes</td><td>Vegetables</td><td>25</td></tr><tr><td>P5</td><td>Cucumbers</td><td>Vegetables</td><td>99</td></tr><tr><td>P6</td><td>Carrots</td><td>Vegetables</td><td>75</td></tr><tr><td>P7</td><td>Bread</td><td>Bakery</td><td>32</td></tr><tr><td>P8</td><td>Milk</td><td>Dairy</td><td>25</td></tr><tr><td>P9</td><td>Cheese</td><td>Dairy</td><td>40</td></tr><tr><td>P10</td><td>Eggs</td><td>Dairy</td><td>8</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "ProductID",
         "ProductName",
         "Category",
         "Price"
        ],
        [
         "P1",
         "Apples",
         "Fruits",
         "199"
        ],
        [
         "P2",
         "Bananas",
         "Fruits",
         "99"
        ],
        [
         "P3",
         "Oranges",
         "Fruits",
         "29"
        ],
        [
         "P4",
         "Tomatoes",
         "Vegetables",
         "25"
        ],
        [
         "P5",
         "Cucumbers",
         "Vegetables",
         "99"
        ],
        [
         "P6",
         "Carrots",
         "Vegetables",
         "75"
        ],
        [
         "P7",
         "Bread",
         "Bakery",
         "32"
        ],
        [
         "P8",
         "Milk",
         "Dairy",
         "25"
        ],
        [
         "P9",
         "Cheese",
         "Dairy",
         "40"
        ],
        [
         "P10",
         "Eggs",
         "Dairy",
         "8"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "_c0",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_c1",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_c2",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_c3",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa36b041-8360-49fb-9c01-6018e00af12d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-172312610099777>:7\u001B[0m\n",
       "\u001B[1;32m      5\u001B[0m     display(df)\n",
       "\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m df\n",
       "\u001B[0;32m----> 7\u001B[0m   _sqldf \u001B[38;5;241m=\u001B[39m \u001B[43m____databricks_percent_sql\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m      9\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m ____databricks_percent_sql\n",
       "\n",
       "File \u001B[0;32m<command-172312610099777>:4\u001B[0m, in \u001B[0;36m____databricks_percent_sql\u001B[0;34m()\u001B[0m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m____databricks_percent_sql\u001B[39m():\n",
       "\u001B[1;32m      3\u001B[0m   \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mbase64\u001B[39;00m\n",
       "\u001B[0;32m----> 4\u001B[0m   df \u001B[38;5;241m=\u001B[39m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase64\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstandard_b64decode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mU0VMRUNUIGMuX2MxIEFTIEZpcnN0TmFtZSwgYy5fYzIgQVMgTGFzdE5hbWUsIFNVTShwLlByaWNlICogb2QuUXVhbnRpdHkpIEFTIFRvdGFsUmV2ZW51ZQpGUk9NIGN1c3RvbWVyIGMKSk9JTiBvcmRlciBvIE9OIGMuX2MwID0gby5DdXN0b21lcklECkpPSU4gb3JkZXJfZGV0YWlscyBvZCBPTiBvLl9jMCA9IG9kLk9yZGVySUQKSk9JTiBwcm9kdWN0IHAgT04gb2QuX2MxID0gcC5fYzAKR1JPVVAgQlkgYy5fYzAsIGMuX2MxLCBjLl9jMg==\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m      5\u001B[0m   display(df)\n",
       "\u001B[1;32m      6\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m df\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1387\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1385\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m   1386\u001B[0m     litArgs \u001B[38;5;241m=\u001B[39m {k: _to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m {})\u001B[38;5;241m.\u001B[39mitems()}\n",
       "\u001B[0;32m-> 1387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n",
       "\u001B[1;32m   1388\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m   1389\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [INVALID_EXTRACT_BASE_FIELD_TYPE] Can't extract a value from \"order\". Need a complex type [STRUCT, ARRAY, MAP] but got \"STRING\"."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-172312610099777>:7\u001B[0m\n\u001B[1;32m      5\u001B[0m     display(df)\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m df\n\u001B[0;32m----> 7\u001B[0m   _sqldf \u001B[38;5;241m=\u001B[39m \u001B[43m____databricks_percent_sql\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m      9\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m ____databricks_percent_sql\n\nFile \u001B[0;32m<command-172312610099777>:4\u001B[0m, in \u001B[0;36m____databricks_percent_sql\u001B[0;34m()\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m____databricks_percent_sql\u001B[39m():\n\u001B[1;32m      3\u001B[0m   \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mbase64\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m   df \u001B[38;5;241m=\u001B[39m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase64\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstandard_b64decode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mU0VMRUNUIGMuX2MxIEFTIEZpcnN0TmFtZSwgYy5fYzIgQVMgTGFzdE5hbWUsIFNVTShwLlByaWNlICogb2QuUXVhbnRpdHkpIEFTIFRvdGFsUmV2ZW51ZQpGUk9NIGN1c3RvbWVyIGMKSk9JTiBvcmRlciBvIE9OIGMuX2MwID0gby5DdXN0b21lcklECkpPSU4gb3JkZXJfZGV0YWlscyBvZCBPTiBvLl9jMCA9IG9kLk9yZGVySUQKSk9JTiBwcm9kdWN0IHAgT04gb2QuX2MxID0gcC5fYzAKR1JPVVAgQlkgYy5fYzAsIGMuX2MxLCBjLl9jMg==\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m   display(df)\n\u001B[1;32m      6\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m df\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1387\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n\u001B[1;32m   1385\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1386\u001B[0m     litArgs \u001B[38;5;241m=\u001B[39m {k: _to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m {})\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[0;32m-> 1387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m   1388\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   1389\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: [INVALID_EXTRACT_BASE_FIELD_TYPE] Can't extract a value from \"order\". Need a complex type [STRUCT, ARRAY, MAP] but got \"STRING\".",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [INVALID_EXTRACT_BASE_FIELD_TYPE] Can't extract a value from \"order\". Need a complex type [STRUCT, ARRAY, MAP] but got \"STRING\".",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT customer._c1 AS FirstName, customer._c2 AS LastName, SUM(product.Price * order_details._c2) AS TotalRevenue\n",
    "FROM customer\n",
    "JOIN order ON customer._c0 = \"order\".CustomerID\n",
    "JOIN order_details ON \"order\"._c0 = order_details.OrderID\n",
    "JOIN product ON order_details._c1 = product._c0\n",
    "GROUP BY customer._c0, customer._c1, customer._c2;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd6debe1-5d2c-421e-b8fe-232da5df3b1d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "From here we can go\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8f7f7b7-d5cf-4bdf-a85f-7302bc4da0b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93a90ec8-5339-4a64-b6d3-8d851c8a44a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_df  = spark.read.csv('dbfs:/FileStore/Customer_Pratice.csv')\n",
    "order_df = spark.read.csv('dbfs:/FileStore/Orders_Pratice.csv')\n",
    "order_details_df = spark.read.csv('dbfs:/FileStore/Order_Details_Pratice.csv')\n",
    "product_df = spark.read.csv('dbfs:/FileStore/Products_Pratice.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41ffa840-f9fe-4bc2-891e-917e671f69d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "customer_df.createOrReplaceTempView(\"customer1\")\n",
    "order_df.createOrReplaceTempView(\"order1\")\n",
    "order_details_df.createOrReplaceTempView(\"order_details1\")\n",
    "product_df.createOrReplaceTempView(\"product1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcaa221e-e615-422c-b1c3-0e772691ecdd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2883937863774685>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m result_df \u001B[38;5;241m=\u001B[39m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\"\"\u001B[39;49m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;124;43m    SELECT c._c1 AS FirstName, c._c2 AS LastName, SUM(p.Price * od._c2) AS TotalRevenue\u001B[39;49m\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;124;43m    FROM custome1r c\u001B[39;49m\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;124;43m    JOIN `order1` o ON c._c0 = o.CustomerID\u001B[39;49m\n",
       "\u001B[1;32m      5\u001B[0m \u001B[38;5;124;43m    JOIN order_details1 od ON o._c0 = od.OrderID\u001B[39;49m\n",
       "\u001B[1;32m      6\u001B[0m \u001B[38;5;124;43m    JOIN product1 p ON od._c1 = p._c0\u001B[39;49m\n",
       "\u001B[1;32m      7\u001B[0m \u001B[38;5;124;43m    GROUP BY c._c0, c._c1, c._c2\u001B[39;49m\n",
       "\u001B[1;32m      8\u001B[0m \u001B[38;5;124;43m\"\"\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1387\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1385\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m   1386\u001B[0m     litArgs \u001B[38;5;241m=\u001B[39m {k: _to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m {})\u001B[38;5;241m.\u001B[39mitems()}\n",
       "\u001B[0;32m-> 1387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n",
       "\u001B[1;32m   1388\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m   1389\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `custome1r` cannot be found. Verify the spelling and correctness of the schema and catalog.\n",
       "If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\n",
       "To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 3 pos 9;\n",
       "'Aggregate ['c._c0, 'c._c1, 'c._c2], ['c._c1 AS FirstName#1031, 'c._c2 AS LastName#1032, 'SUM(('p.Price * 'od._c2)) AS TotalRevenue#1033]\n",
       "+- 'Join Inner, ('od._c1 = 'p._c0)\n",
       "   :- 'Join Inner, ('o._c0 = 'od.OrderID)\n",
       "   :  :- 'Join Inner, ('c._c0 = 'o.CustomerID)\n",
       "   :  :  :- 'SubqueryAlias c\n",
       "   :  :  :  +- 'UnresolvedRelation [custome1r], [], false\n",
       "   :  :  +- SubqueryAlias o\n",
       "   :  :     +- SubqueryAlias order1\n",
       "   :  :        +- View (`order1`, [_c0#801,_c1#802,_c2#803,_c3#804])\n",
       "   :  :           +- Relation [_c0#801,_c1#802,_c2#803,_c3#804] csv\n",
       "   :  +- SubqueryAlias od\n",
       "   :     +- SubqueryAlias order_details1\n",
       "   :        +- View (`order_details1`, [_c0#826,_c1#827,_c2#828])\n",
       "   :           +- Relation [_c0#826,_c1#827,_c2#828] csv\n",
       "   +- SubqueryAlias p\n",
       "      +- SubqueryAlias product1\n",
       "         +- View (`product1`, [_c0#849,_c1#850,_c2#851,_c3#852])\n",
       "            +- Relation [_c0#849,_c1#850,_c2#851,_c3#852] csv\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-2883937863774685>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m result_df \u001B[38;5;241m=\u001B[39m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\"\"\u001B[39;49m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;124;43m    SELECT c._c1 AS FirstName, c._c2 AS LastName, SUM(p.Price * od._c2) AS TotalRevenue\u001B[39;49m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;124;43m    FROM custome1r c\u001B[39;49m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;124;43m    JOIN `order1` o ON c._c0 = o.CustomerID\u001B[39;49m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;124;43m    JOIN order_details1 od ON o._c0 = od.OrderID\u001B[39;49m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;124;43m    JOIN product1 p ON od._c1 = p._c0\u001B[39;49m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;124;43m    GROUP BY c._c0, c._c1, c._c2\u001B[39;49m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;124;43m\"\"\"\u001B[39;49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1387\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n\u001B[1;32m   1385\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1386\u001B[0m     litArgs \u001B[38;5;241m=\u001B[39m {k: _to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m {})\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[0;32m-> 1387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m   1388\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   1389\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `custome1r` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 3 pos 9;\n'Aggregate ['c._c0, 'c._c1, 'c._c2], ['c._c1 AS FirstName#1031, 'c._c2 AS LastName#1032, 'SUM(('p.Price * 'od._c2)) AS TotalRevenue#1033]\n+- 'Join Inner, ('od._c1 = 'p._c0)\n   :- 'Join Inner, ('o._c0 = 'od.OrderID)\n   :  :- 'Join Inner, ('c._c0 = 'o.CustomerID)\n   :  :  :- 'SubqueryAlias c\n   :  :  :  +- 'UnresolvedRelation [custome1r], [], false\n   :  :  +- SubqueryAlias o\n   :  :     +- SubqueryAlias order1\n   :  :        +- View (`order1`, [_c0#801,_c1#802,_c2#803,_c3#804])\n   :  :           +- Relation [_c0#801,_c1#802,_c2#803,_c3#804] csv\n   :  +- SubqueryAlias od\n   :     +- SubqueryAlias order_details1\n   :        +- View (`order_details1`, [_c0#826,_c1#827,_c2#828])\n   :           +- Relation [_c0#826,_c1#827,_c2#828] csv\n   +- SubqueryAlias p\n      +- SubqueryAlias product1\n         +- View (`product1`, [_c0#849,_c1#850,_c2#851,_c3#852])\n            +- Relation [_c0#849,_c1#850,_c2#851,_c3#852] csv\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `custome1r` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 3 pos 9;\n'Aggregate ['c._c0, 'c._c1, 'c._c2], ['c._c1 AS FirstName#1031, 'c._c2 AS LastName#1032, 'SUM(('p.Price * 'od._c2)) AS TotalRevenue#1033]\n+- 'Join Inner, ('od._c1 = 'p._c0)\n   :- 'Join Inner, ('o._c0 = 'od.OrderID)\n   :  :- 'Join Inner, ('c._c0 = 'o.CustomerID)\n   :  :  :- 'SubqueryAlias c\n   :  :  :  +- 'UnresolvedRelation [custome1r], [], false\n   :  :  +- SubqueryAlias o\n   :  :     +- SubqueryAlias order1\n   :  :        +- View (`order1`, [_c0#801,_c1#802,_c2#803,_c3#804])\n   :  :           +- Relation [_c0#801,_c1#802,_c2#803,_c3#804] csv\n   :  +- SubqueryAlias od\n   :     +- SubqueryAlias order_details1\n   :        +- View (`order_details1`, [_c0#826,_c1#827,_c2#828])\n   :           +- Relation [_c0#826,_c1#827,_c2#828] csv\n   +- SubqueryAlias p\n      +- SubqueryAlias product1\n         +- View (`product1`, [_c0#849,_c1#850,_c2#851,_c3#852])\n            +- Relation [_c0#849,_c1#850,_c2#851,_c3#852] csv\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "result_df = spark.sql(\"\"\"\n",
    "    SELECT c._c1 AS FirstName, c._c2 AS LastName, SUM(p.Price * od._c2) AS TotalRevenue\n",
    "    FROM custome1r c\n",
    "    JOIN `order1` o ON c._c0 = o.CustomerID\n",
    "    JOIN order_details1 od ON o._c0 = od.OrderID\n",
    "    JOIN product1 p ON od._c1 = p._c0\n",
    "    GROUP BY c._c0, c._c1, c._c2\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3966db9a-c686-4d1a-99cd-4fd95a3cbe8f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n|FirstName|LastName|\n+---------+--------+\n|FirstName|LastName|\n|     John|   Smith|\n|     Jane|     Doe|\n|  Michael| Johnson|\n|    Sarah|     Lee|\n|    David|     Kim|\n|    wonda|    long|\n|    shuri|     wun|\n|      tom| holland|\n+---------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "734dfbcf-6bff-49de-a124-a154b636f7f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2883937863774688>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m result_df \u001B[38;5;241m=\u001B[39m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\"\"\u001B[39;49m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;124;43m    SELECT c._c1 AS FirstName, c._c2 AS LastName, SUM(p.Price * od._c2) AS TotalRevenue\u001B[39;49m\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;124;43m    FROM customer c\u001B[39;49m\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;124;43m    JOIN `order` o ON c._c0 = o.CustomerID\u001B[39;49m\n",
       "\u001B[1;32m      5\u001B[0m \u001B[38;5;124;43m    JOIN order_details od ON o._c0 = od.OrderID\u001B[39;49m\n",
       "\u001B[1;32m      6\u001B[0m \u001B[38;5;124;43m    JOIN product p ON od._c1 = p._c0\u001B[39;49m\n",
       "\u001B[1;32m      7\u001B[0m \u001B[38;5;124;43m    GROUP BY c._c0, c._c1, c._c2\u001B[39;49m\n",
       "\u001B[1;32m      8\u001B[0m \u001B[38;5;124;43m\"\"\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1387\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1385\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m   1386\u001B[0m     litArgs \u001B[38;5;241m=\u001B[39m {k: _to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m {})\u001B[38;5;241m.\u001B[39mitems()}\n",
       "\u001B[0;32m-> 1387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n",
       "\u001B[1;32m   1388\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m   1389\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `o`.`CustomerID` cannot be resolved. Did you mean one of the following? [`o`.`_c0`, `o`.`_c1`, `o`.`_c2`, `o`.`_c3`, `c`.`_c0`].; line 4 pos 30;\n",
       "'Aggregate ['c._c0, 'c._c1, 'c._c2], ['c._c1 AS FirstName#1034, 'c._c2 AS LastName#1035, 'SUM(('p.Price * 'od._c2)) AS TotalRevenue#1036]\n",
       "+- 'Join Inner, ('od._c1 = 'p._c0)\n",
       "   :- 'Join Inner, ('o._c0 = 'od.OrderID)\n",
       "   :  :- 'Join Inner, (_c0#93 = 'o.CustomerID)\n",
       "   :  :  :- SubqueryAlias c\n",
       "   :  :  :  +- SubqueryAlias customer\n",
       "   :  :  :     +- View (`customer`, [_c0#93,_c1#94,_c2#95,_c3#96,_c4#97])\n",
       "   :  :  :        +- Relation [_c0#93,_c1#94,_c2#95,_c3#96,_c4#97] csv\n",
       "   :  :  +- SubqueryAlias o\n",
       "   :  :     +- SubqueryAlias order\n",
       "   :  :        +- View (`order`, [_c0#120,_c1#121,_c2#122,_c3#123])\n",
       "   :  :           +- Relation [_c0#120,_c1#121,_c2#122,_c3#123] csv\n",
       "   :  +- SubqueryAlias od\n",
       "   :     +- SubqueryAlias order_details\n",
       "   :        +- View (`order_details`, [_c0#145,_c1#146,_c2#147])\n",
       "   :           +- Relation [_c0#145,_c1#146,_c2#147] csv\n",
       "   +- SubqueryAlias p\n",
       "      +- SubqueryAlias product\n",
       "         +- View (`product`, [_c0#168,_c1#169,_c2#170,_c3#171])\n",
       "            +- Relation [_c0#168,_c1#169,_c2#170,_c3#171] csv\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-2883937863774688>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m result_df \u001B[38;5;241m=\u001B[39m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\"\"\u001B[39;49m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;124;43m    SELECT c._c1 AS FirstName, c._c2 AS LastName, SUM(p.Price * od._c2) AS TotalRevenue\u001B[39;49m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;124;43m    FROM customer c\u001B[39;49m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;124;43m    JOIN `order` o ON c._c0 = o.CustomerID\u001B[39;49m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;124;43m    JOIN order_details od ON o._c0 = od.OrderID\u001B[39;49m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;124;43m    JOIN product p ON od._c1 = p._c0\u001B[39;49m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;124;43m    GROUP BY c._c0, c._c1, c._c2\u001B[39;49m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;124;43m\"\"\"\u001B[39;49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1387\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n\u001B[1;32m   1385\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1386\u001B[0m     litArgs \u001B[38;5;241m=\u001B[39m {k: _to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m {})\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[0;32m-> 1387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m   1388\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   1389\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `o`.`CustomerID` cannot be resolved. Did you mean one of the following? [`o`.`_c0`, `o`.`_c1`, `o`.`_c2`, `o`.`_c3`, `c`.`_c0`].; line 4 pos 30;\n'Aggregate ['c._c0, 'c._c1, 'c._c2], ['c._c1 AS FirstName#1034, 'c._c2 AS LastName#1035, 'SUM(('p.Price * 'od._c2)) AS TotalRevenue#1036]\n+- 'Join Inner, ('od._c1 = 'p._c0)\n   :- 'Join Inner, ('o._c0 = 'od.OrderID)\n   :  :- 'Join Inner, (_c0#93 = 'o.CustomerID)\n   :  :  :- SubqueryAlias c\n   :  :  :  +- SubqueryAlias customer\n   :  :  :     +- View (`customer`, [_c0#93,_c1#94,_c2#95,_c3#96,_c4#97])\n   :  :  :        +- Relation [_c0#93,_c1#94,_c2#95,_c3#96,_c4#97] csv\n   :  :  +- SubqueryAlias o\n   :  :     +- SubqueryAlias order\n   :  :        +- View (`order`, [_c0#120,_c1#121,_c2#122,_c3#123])\n   :  :           +- Relation [_c0#120,_c1#121,_c2#122,_c3#123] csv\n   :  +- SubqueryAlias od\n   :     +- SubqueryAlias order_details\n   :        +- View (`order_details`, [_c0#145,_c1#146,_c2#147])\n   :           +- Relation [_c0#145,_c1#146,_c2#147] csv\n   +- SubqueryAlias p\n      +- SubqueryAlias product\n         +- View (`product`, [_c0#168,_c1#169,_c2#170,_c3#171])\n            +- Relation [_c0#168,_c1#169,_c2#170,_c3#171] csv\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `o`.`CustomerID` cannot be resolved. Did you mean one of the following? [`o`.`_c0`, `o`.`_c1`, `o`.`_c2`, `o`.`_c3`, `c`.`_c0`].; line 4 pos 30;\n'Aggregate ['c._c0, 'c._c1, 'c._c2], ['c._c1 AS FirstName#1034, 'c._c2 AS LastName#1035, 'SUM(('p.Price * 'od._c2)) AS TotalRevenue#1036]\n+- 'Join Inner, ('od._c1 = 'p._c0)\n   :- 'Join Inner, ('o._c0 = 'od.OrderID)\n   :  :- 'Join Inner, (_c0#93 = 'o.CustomerID)\n   :  :  :- SubqueryAlias c\n   :  :  :  +- SubqueryAlias customer\n   :  :  :     +- View (`customer`, [_c0#93,_c1#94,_c2#95,_c3#96,_c4#97])\n   :  :  :        +- Relation [_c0#93,_c1#94,_c2#95,_c3#96,_c4#97] csv\n   :  :  +- SubqueryAlias o\n   :  :     +- SubqueryAlias order\n   :  :        +- View (`order`, [_c0#120,_c1#121,_c2#122,_c3#123])\n   :  :           +- Relation [_c0#120,_c1#121,_c2#122,_c3#123] csv\n   :  +- SubqueryAlias od\n   :     +- SubqueryAlias order_details\n   :        +- View (`order_details`, [_c0#145,_c1#146,_c2#147])\n   :           +- Relation [_c0#145,_c1#146,_c2#147] csv\n   +- SubqueryAlias p\n      +- SubqueryAlias product\n         +- View (`product`, [_c0#168,_c1#169,_c2#170,_c3#171])\n            +- Relation [_c0#168,_c1#169,_c2#170,_c3#171] csv\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "result_df = spark.sql(\"\"\"\n",
    "    SELECT c._c1 AS FirstName, c._c2 AS LastName, SUM(p.Price * od._c2) AS TotalRevenue\n",
    "    FROM customer c\n",
    "    JOIN `order` o ON c._c0 = o.CustomerID\n",
    "    JOIN order_details od ON o._c0 = od.OrderID\n",
    "    JOIN product p ON od._c1 = p._c0\n",
    "    GROUP BY c._c0, c._c1, c._c2\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1e89b58-4ae2-4a8e-aced-2539eac38665",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3125408404689997,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Bhupendra_Vaishnav_PIET_WEEK6",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
