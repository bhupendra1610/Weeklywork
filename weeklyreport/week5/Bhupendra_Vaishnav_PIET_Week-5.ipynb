{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7745a6dd-db66-4525-bc76-67d534df3b19",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#####Task1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78b8cc19-44f8-4c58-bcb0-91dbfa5f7c13",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#making dataframe\n",
    "from pyspark.sql import SparkSession\n",
    "spark =SparkSession.builder.appName('Task1.com').getOrCreate()\n",
    "data = [ (1, \"bhupendra\",\"vaishnav\",\"A\",\"42\"),\n",
    "        (2, \"rahul\",\"singh\",\"B\",\"21\"),\n",
    "        (3, \"rashma\",\"khan\",\"C\",\"14\"),\n",
    "        (4 ,\"sahil\",\"sharma\",\"A\",\"52\")]\n",
    "schema = [\"id\",\"firstname\",\"lastname\",\"section\",\"rollno\"]\n",
    "df = spark.createDataFrame(data=data,schema = schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7774fb7b-f88b-4fe8-b182-2a0af7b42692",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------+-------+------+\n| id|firstname|lastname|section|rollno|\n+---+---------+--------+-------+------+\n|  1|bhupendra|vaishnav|      A|    42|\n|  2|    rahul|   singh|      B|    21|\n|  3|   rashma|    khan|      C|    14|\n|  4|    sahil|  sharma|      A|    52|\n+---+---------+--------+-------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d6dd208-0e38-484f-a2bc-bca7e76516fd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#####1. Update The Value of an Existing Column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbb4e7dc-c47e-4450-a51c-7eea9145a5b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import col\n",
    "new_df = df.withColumn(\"updated_rollno\",df.rollno*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1fcb769-2024-4788-b940-fceac1f06e8f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------+-------+------+--------------+\n| id|firstname|lastname|section|rollno|updated_rollno|\n+---+---------+--------+-------+------+--------------+\n|  1|bhupendra|vaishnav|      A|    42|          84.0|\n|  2|    rahul|   singh|      B|    21|          42.0|\n|  3|   rashma|    khan|      C|    14|          28.0|\n|  4|    sahil|  sharma|      A|    52|         104.0|\n+---+---------+--------+-------+------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "#updated value of the rollno\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22cb4dea-a45b-4d74-9ad3-c9eb6456aac0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 2. Change data type using withcolumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02a01910-efc5-40df-ad93-886b1823caeb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: long (nullable = true)\n |-- firstname: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- section: string (nullable = true)\n |-- rollno: string (nullable = true)\n\nroot\n |-- id: long (nullable = true)\n |-- firstname: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- section: string (nullable = true)\n |-- rollno: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.printSchema()\n",
    "new_df=df.withColumn(\"rollno\",df.rollno.cast(\"Integer\"))\n",
    "new_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b2013cf-f9e4-4487-a7b5-54ae7e58ea21",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#####Task2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b6641ac-85c8-494f-9def-68fc10285ebf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#####1.Using row class create a dataframe having nested structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4f872ac-236e-42bd-a45a-eaae0ccc55fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from pyspark.sql import SparkSession, Row\n",
    "row1 = Row(\"bhupendra\",22,Row(\"sawai madhopur\",\"Rajasthan\"))\n",
    "row2 = Row(\"rahul\",32,Row(\"Bhopal\",\"MP\"))\n",
    "row3 = Row(\"rohan\",23,Row(\"dwarika\",\"gujarat\"))\n",
    "row = [row1,row2,row3]\n",
    "column = [\"name\", \"age\", \"address\"]\n",
    "df = spark.createDataFrame(row,column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f98cc1b1-5a0a-44f9-bcfa-234f198d38f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+--------------------+\n|     name|age|             address|\n+---------+---+--------------------+\n|bhupendra| 22|{sawai madhopur, ...|\n|    rahul| 32|        {Bhopal, MP}|\n|    rohan| 23|  {dwarika, gujarat}|\n+---------+---+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "374bce9a-9507-4be9-a6c7-d7d5f47c44d0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#####Task3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b95948d-403f-46cd-9ed9-68676cc4a165",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#####1.To use the Column class, you need to import it from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43e93122-3389-47bb-be4a-d3ce2c9b4d45",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "col_obj =  lit(\"Task1.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bf0143d-dc24-4881-9073-714009b24d81",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- name: string (nullable = true)\n |-- age: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "data = [(\"bhupendra\",21), (\"james\",24)]\n",
    "df = spark.createDataFrame(data).toDF(\"name\",\"age\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d196025-85f7-42e2-b377-f7cc9228e485",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n|age|\n+---+\n| 21|\n| 24|\n+---+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(df.age).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4ced4e8-4ebf-4042-87b1-5cd311466d02",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n|age|\n+---+\n| 21|\n| 24|\n+---+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(df['age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53cc2963-d430-4e63-b0fb-39b60e3e1d42",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n|age|\n+---+\n| 21|\n| 24|\n+---+\n\n"
     ]
    }
   ],
   "source": [
    "#by using the col () function\n",
    "from pyspark.sql.functions import col\n",
    "df.select(col(\"age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc12a01f-315b-4a98-9521-bcfed80bd0f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n|     name|\n+---------+\n|bhupendra|\n|    james|\n+---------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"name\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34f18e50-3456-48a6-a622-4e07c8fb4e4c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#####2.create Column objects by using various DataFrame API functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e54b6820-54e0-4b36-8fcd-8328c4d701ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "data = [(100,12,\"Rahul\"),(12,123, \"bhupendra\")]\n",
    "column = [ \"number\", \"rollno\",\"name\"]\n",
    "df = spark.createDataFrame(data,column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93bf2c40-3b76-4b74-9db5-50ae65805a23",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#using col() function\n",
    "name = col('name')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d489ca68-27c4-4959-9e15-9e42ce54f784",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n|(number + rollno)|\n+-----------------+\n|              112|\n|              135|\n+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "#using sum operations\n",
    "sum =df.select( df['number']+df['rollno']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9727a2df-bd22-4f7f-8a8b-c0060924fb61",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n|     name|\n+---------+\n|    Rahul|\n|bhupendra|\n+---------+\n\n"
     ]
    }
   ],
   "source": [
    "#using indexing\n",
    "first_col = df.select(df['name']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce94bc8c-64dd-48a1-9485-7b1e75ec8515",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n|(rollno > 10)|\n+-------------+\n|         true|\n|         true|\n+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "#using comparison operations\n",
    "condition = df.select(df['rollno']>10).show(#)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c461498-9219-4a8e-9ac7-c3045671596d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n|avg(rollno)|\n+-----------+\n|       67.5|\n+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "#using the aggregate function\n",
    "from pyspark.sql.functions import avg\n",
    "avg_col = df.select(avg(df[\"rollno\"])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9b169db-2caf-41f1-80f2-2505d433745f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n|concat(name, , rollno)|\n+----------------------+\n|               Rahul12|\n|          bhupendra123|\n+----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# string manipulation functions\n",
    "from pyspark.sql.functions import concat,lit\n",
    "concat_col = df.select(concat(df['name'] , lit(''),df['rollno'])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a67ae3e4-78fd-4e4a-b8e3-e3ce9aa962b9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#####Task4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3ff97fb-1668-4730-b184-b10dada5631a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#####1.if age is greater than 18 then multiply salary by 2 if not then make that column as null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3010db8-0e55-4c9b-a10b-250b75d9127a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+------+\n|     name|age|salary|\n+---------+---+------+\n|bhupendra| 21| 81000|\n|    rahul| 19| 83999|\n|    rohan| 18| 84000|\n|   rashma| 17| 85000|\n+---------+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "#creating a dataframe\n",
    "data  = [(\"bhupendra\",21, 81000),\n",
    "          (\"rahul\", 19 , 83999),\n",
    "         (\"rohan\" , 18 , 84000 ),\n",
    "         (\"rashma\",17 , 85000)\n",
    "        ]\n",
    "schema = [\"name\",\"age\",\"salary\"]\n",
    "new_df = spark.createDataFrame(data=data ,schema =schema)\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3cddaf0-cfba-45f6-8130-81567d4774c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "new_df1  = new_df.withColumn(\"updated_salary\" , when(new_df.age >18 , new_df.salary*2)\n",
    "                                     .otherwise(\"null\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecba73c4-6fc7-44b2-82ae-da016049743c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+------+--------------+\n|     name|age|salary|updated_salary|\n+---------+---+------+--------------+\n|bhupendra| 21| 81000|        162000|\n|    rahul| 19| 83999|        167998|\n|    rohan| 18| 84000|          null|\n|   rashma| 17| 85000|          null|\n+---------+---+------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "new_df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "378918ec-4873-42ac-9cab-8e42e669f4b6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#####2. use when and otherwise with aggregrate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d3d67b9-c3ff-4166-859c-07221f1603b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n|sum_of_salary|\n+-------------+\n|       337000|\n+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#sum\n",
    "from pyspark.sql.functions import sum, when\n",
    "sum_salary = sum( when(new_df['age'] >18, 84000) .otherwise(new_df['salary'])) \n",
    "result_sum =new_df.select(sum_salary.alias(\"sum_of_salary\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8d42e87-30a6-4e58-ba98-f3faf903b57a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n|avg(salary)|\n+-----------+\n|   83499.75|\n+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "#average\n",
    "from pyspark.sql.functions import avg\n",
    "avg_sum = new_df.select(avg(col('salary'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d18225d-0463-4a7f-8e1e-c9d93cadea23",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#####Task5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f82f96c-6b66-4168-a305-806c2de6c96e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#creating the new data frame \n",
    "\n",
    "data = [(\"Bhupendra\" , \"Male\" ,21, 51000 ),\n",
    "       (\"Rishi\" , \"Male\" ,24, 31000 ),\n",
    "        (\"Sahil\" , \"Male\" ,22, 51303 ),\n",
    "        (\"Roni\" , \"Female\" ,23, 51000 ),\n",
    "        (\"Rashma\" , \"Female\" ,19, 41300 ),\n",
    "        (\"Raghav\" , \"Male\" ,\"\" , 41300 ),\n",
    "        \n",
    "       ]\n",
    "column = [\"Name\",\"Gender\",\"age\",\"salary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "346ae4ef-8266-418f-afa7-1e530cd90331",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_new = spark.createDataFrame(data,column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfb38661-ae61-4d23-aa1b-2ee87e35d776",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+---+------+\n|     Name|Gender|age|salary|\n+---------+------+---+------+\n|Bhupendra|  Male| 21| 51000|\n|    Rishi|  Male| 24| 31000|\n|    Sahil|  Male| 22| 51303|\n|     Roni|Female| 23| 51000|\n|   Rashma|Female| 19| 41300|\n|   Raghav|  Male|   | 41300|\n+---------+------+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "df_new.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88ebe354-6486-47a5-a3b7-df818ad56699",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "######1 Rename the Name to Employee name with out using the withColumn renamed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de6b2c11-5c5e-47b2-b06b-a5c7e6b8c551",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#using alias\n",
    "#renaming a col using alias\n",
    "df_new1 =df_new.select(df_new.Name.alias(\"Employee_Name\"),df_new.Gender,df_new.age,df_new.salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "495b8c3e-178e-468b-9243-557ea2e29908",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+---+------+\n|Employee_Name|Gender|age|salary|\n+-------------+------+---+------+\n|    Bhupendra|  Male| 21| 51000|\n|        Rishi|  Male| 24| 31000|\n|        Sahil|  Male| 22| 51303|\n|         Roni|Female| 23| 51000|\n|       Rashma|Female| 19| 41300|\n|       Raghav|  Male|   | 41300|\n+-------------+------+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "df_new1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50682ede-c1ea-40f6-9a2d-d12d4a0d634d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#####2. Sort a column in desc order having null values at the end and then sort a column in asc order having null values at beg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47ef7bb0-cf1a-4c81-ad67-82f37b3195bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+---+------+\n|     Name|Gender|age|salary|\n+---------+------+---+------+\n|   Raghav|  Male|   | 41300|\n|   Rashma|Female| 19| 41300|\n|Bhupendra|  Male| 21| 51000|\n|    Sahil|  Male| 22| 51303|\n|     Roni|Female| 23| 51000|\n|    Rishi|  Male| 24| 31000|\n+---------+------+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "#sort in asc order having null values at begining\n",
    "df_new2=  df_new.sort(df_new.age.asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70c7be35-ac05-4c8e-bc6b-31c831a1ec4b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+---+------+\n|     Name|Gender|age|salary|\n+---------+------+---+------+\n|    Rishi|  Male| 24| 31000|\n|     Roni|Female| 23| 51000|\n|    Sahil|  Male| 22| 51303|\n|Bhupendra|  Male| 21| 51000|\n|   Rashma|Female| 19| 41300|\n|   Raghav|  Male|   | 41300|\n+---------+------+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#sort in desc order having null values at end\n",
    "df_new3=  df_new.sort(df_new.age.desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3968749-ecff-492c-af8e-fe2623b56cb6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#####Task6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ec94f75-36d9-4e61-a546-2cb783fa313b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#####1. Display only name and age whose gender is 'Male' or 'MALE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cd7e016-1ef9-4312-a7ce-880acc3537dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+---+------+\n|     Name|Gender|age|salary|\n+---------+------+---+------+\n|Bhupendra|  Male| 21| 51000|\n|    Rishi|  Male| 24| 31000|\n|    Sahil|  Male| 22| 51303|\n|   Raghav|  Male|   | 41300|\n+---------+------+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "df_new4 = df_new.filter(col(\"Gender\").like(\"Male%\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27595fea-01bb-433a-b3e4-3c93881829c1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#####Task7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "655a45eb-0f30-4650-b2a4-46e1d31f2d73",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#####1 Distinct() & DropDuplicates() In Pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3732622c-8cad-43d2-8a26-f056bba66cc7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#disctinct() function  : It is a transformation operation that returns a new DataFrame with unique rows. It considers all columns of the DataFrame.\n",
    "\n",
    "# distinct_df = df.distinct()\n",
    "\n",
    "#dropDuplicate() function  : It is also a transformation operation that returns a new DataFrame with duplicate rows removed. However, it provides more flexibility by allowing you to specify the subset of columns to consider for duplicate elimination.\n",
    "\n",
    "#drop_duplicate_df = df.dropDuplicates(['col1','col2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d006f53-38e7-4250-b558-5fce2e35578b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#####2. In what scenarios is it appropriate to use the dropDuplicates() , and could you provide an example that demonstrates its usage?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9aa917d-ef93-4af1-b428-35109c3a74bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+---+------+\n|     Name|Gender|age|salary|\n+---------+------+---+------+\n|Bhupendra|  Male| 21| 51000|\n|    Rishi|  Male| 24| 31000|\n|    Sahil|  Male| 22| 51303|\n|     Roni|Female| 23| 51000|\n|   Rashma|Female| 19| 41300|\n|   Raghav|  Male|   | 41300|\n|    Rishi|  Male| 24| 31000|\n+---------+------+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "#it is appropriate to use dropDuplicate wher we need specific column  should be removed\n",
    "# few scenarios are\n",
    "#data cleaning\n",
    "#data deduplication\n",
    "#join operations\n",
    "\n",
    "#example\n",
    "newRow=  [(\"Rishi\",\"Male\",24,31000)]\n",
    "addRow = spark.createDataFrame(newRow,column)\n",
    "appended = df_new.union(addRow)\n",
    "appended.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fb94355-4370-472e-b5a7-6fc80411f691",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+---+------+\n|     Name|Gender|age|salary|\n+---------+------+---+------+\n|Bhupendra|  Male| 21| 51000|\n|    Rishi|  Male| 24| 31000|\n|    Sahil|  Male| 22| 51303|\n|     Roni|Female| 23| 51000|\n|   Rashma|Female| 19| 41300|\n|   Raghav|  Male|   | 41300|\n+---------+------+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "#distinct()\n",
    "df_distinct =df_new.distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad9acb28-88b3-43ad-a363-3cf113145260",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+---+------+\n|     Name|Gender|age|salary|\n+---------+------+---+------+\n|     Roni|Female| 23| 51000|\n|Bhupendra|  Male| 21| 51000|\n+---------+------+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "#dropDuplicates()\n",
    "df_dropDup = df_new.dropDuplicates(['Gender']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d090a4a5-3284-4567-888a-047e7051b6f8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#####Task8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ab38df6-a504-4a76-b07d-4b2884993de3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#####1. How to sort a dataframe based on the columns \"Country,\" \"State,\" and \"City\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b9c22e0-90ac-40c6-bdf3-122f9e867b84",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+-------------+\n|       country|     state|         city|\n+--------------+----------+-------------+\n|         India| Rajasthan|       Jaipur|\n|         India| Rajasthan|SawaiMadhopur|\n| United States|California|San Francisco|\n|        Canada|   Ontario|      Toronto|\n|United Kingdom|   England|       London|\n+--------------+----------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "data2 = [(\"India\" , \"Rajasthan\", \"Jaipur\"),\n",
    "        \n",
    "        (\"India\" , \"Rajasthan\", \"SawaiMadhopur\"),\n",
    "         (\"United States\", \"California\", \"San Francisco\"),\n",
    "         (\"Canada\", \"Ontario\" , \"Toronto\"),\n",
    "         (\"United Kingdom\" , \"England\",\"London\")\n",
    "        \n",
    "        ]\n",
    "column = [\"country\",\"state\",\"city\"]\n",
    "df= spark.createDataFrame(data2,column)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5df423a7-7b3d-4942-8ca4-029cbbac397b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+-------------+\n|       country|     state|         city|\n+--------------+----------+-------------+\n|        Canada|   Ontario|      Toronto|\n|         India| Rajasthan|       Jaipur|\n|         India| Rajasthan|SawaiMadhopur|\n|United Kingdom|   England|       London|\n| United States|California|San Francisco|\n+--------------+----------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "sorted_df = df.orderBy(col('country'), col('state'),col('city')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92cf5cc3-1c31-459e-ac1b-a2302555f3d1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Task9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10dfb22c-bd4a-4afe-b159-4feac00ad397",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#####1. Union() and UnionAll() How it is different from that is SQL,merge dataframe without duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "358c7401-98c6-4519-9d11-1c5e229bceae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# union() :function in PySpark performs a distinct union, removing duplicate rows from the result. It ensures that the resulting DataFrame contains only unique rows.\n",
    "\n",
    "# The UNION operation in SQL also removes duplicate rows by default, similar to the union() function in PySpark. It implicitly performs deduplication of rows.\n",
    "\n",
    "# unionAll() : function combines the DataFrames including all rows, including duplicates.\n",
    "# It returns a new DataFrame that includes all rows from both DataFrames, preserving duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09184661-8131-4007-bc9e-e829d217dca3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data1 = [(1,\"bhupendra\",50),\n",
    "         (2,\"rohan\",24),\n",
    "        ]\n",
    "schema1  = [\"id\",\"name\",\"rollno\"]\n",
    "data2 = [(2 ,\"rohan\" ,24),\n",
    "        (4, \"rashma\", 21)]\n",
    "schema2 = [\"id\",\"name\",\"rollno\"]\n",
    "df1 = spark.createDataFrame(data1,schema1)\n",
    "df2 = spark.createDataFrame(data2,schema2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "863e4f42-0008-40ba-ba9e-4a76b72f0b48",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+------+\n| id|     name|rollno|\n+---+---------+------+\n|  1|bhupendra|    50|\n|  2|    rohan|    24|\n+---+---------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e19e6ece-ca22-4016-8802-a813be8a382a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n| id|  name|rollno|\n+---+------+------+\n|  2| rohan|    24|\n|  4|rashma|    21|\n+---+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cd31300-eb64-4440-99b1-d885e3e25d40",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n| id|   name|\n+---+-------+\n|  1|  Alice|\n|  2|    Bob|\n|  3|Charlie|\n+---+-------+\n\n"
     ]
    }
   ],
   "source": [
    "union_df = df1.union(df2)\n",
    "union_df.distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9ecb725-11ee-46a6-97da-dfe5ca1c0ebe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n| id|   name|\n+---+-------+\n|  1|  Alice|\n|  2|    Bob|\n|  2|    Bob|\n|  3|Charlie|\n+---+-------+\n\n"
     ]
    }
   ],
   "source": [
    "unionall_df = df1.unionAll(df2)\n",
    "unionall_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fe9e300-79ce-4c6d-b7da-057d879a2928",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#####Task10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33a9c95b-4ae1-4731-a3d4-56234bdbd7aa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#####1.How can I handle null values or missing data when using groupBy in Spark?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36a129ed-fb54-40ad-a197-67583227b94c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n|     dep|count|\n+--------+-----+\n|Teaching|    1|\n|      IT|    2|\n|      HR|    2|\n| Medical|    1|\n+--------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#groupBy()\n",
    "data = [(1,\"rahul\", \"M\",\"Teaching\",5000),\n",
    "       (2,\"bhupendra\",\"M\",\"IT\",10000),\n",
    "        (3,\"rashma\",\"F\",\"HR\",12000),\n",
    "        (4,\"rohit\",\"M\",\"HR\",13000),\n",
    "        (5,\"sahil\" , \"M\" ,\"IT\" ,12000),\n",
    "         (6,\"ayesha\" ,\"F\", \"Medical\",23000)]\n",
    "\n",
    "\n",
    "schema = [\"id\",\"name\",\"gender\",\"dep\",\"salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data,schema)\n",
    "df.groupBy('dep').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4de2e625-b0a0-40b0-a0f2-1355b305bc7e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n|     name|count|\n+---------+-----+\n|    rahul|    1|\n|bhupendra|    1|\n|   rashma|    1|\n|    rohit|    1|\n|    sahil|    1|\n|   ayesha|    1|\n+---------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df_filter =df.filter(col('name').isNotNull())\n",
    "grouped_df =df_filter.groupBy('name').count().show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Bhupendra_Vaishnav_PIET_Week5",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
